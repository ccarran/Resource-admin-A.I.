{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resource administrator Artificial Intelligence\n",
    "\n",
    "With the rise of Artificial Intelligence thanks to a lot of advancements being made through the last two decades, we've been finding ways to implement it to a lot of aspects of our lifes: From our video recomendations, to our research on fields like astronomy and medicine; from funny robots that can learn to kind of speak like a real human, to advanced AIs that can make a three dimensional space out of a two dimensional picture. And amidst all of these different applications, what I personally find the most interesting is the prospect of giving AIs the ability to tackle on those problems that, while obviously complicated in a real-life environment with hundreds of factors, at their core are actually mostly mechanical, and could potentially be solved using algorithms made by computers.\n",
    "\n",
    "This is the scenario where I came up with the present project. From the natural resources of a country, to the economy of a small family, to the human resources of a business; the role of managing some form of resource is almost inescapable to humans, so I asked the question of to what extend an AI could be trained to learn how to manage some resources in the confined environment of a game, designed and coded by myself. This project doesn't intend to make an AI that could actually be used in a real life situation, but to explore the ability of an AI made using the currently available resources to tackle on this issue.\n",
    "\n",
    "### Overview of the game\n",
    "\n",
    "In the present game, the player is tasked with the mission to manage the resources of a spaceship to try and survive for as long as possible without running out of power, gas or crewmembers. \n",
    "\n",
    "Each playthrough is divided in turns, and in every turn four things happen: The resource being used as fuel is lowered, the crewmembers' hunger and thirst levels are lowered, a random event happens, and, depending on the turn, the player might have to make a decision on which resource is used as fuel or if the crew should eat.\n",
    "\n",
    "#### Main Resources\n",
    "There are four main resources that the player needs to manage: Food, Water, Gas and Power. Food and Water are used to feed the crewmembers during the Meal events, and Gas and Power are used as fuel for the ship every turn, according to the player's decision on the Propulsion Method Choice event. The player starts with 100 units of each of these resources, and can resupply them in some of the Random events that happen every turn.\n",
    "\n",
    "#### Crewmembers\n",
    "The crewmembers are the fifth, unofficial, resource that the player has to manage. Initially the player gets 6 crewmembers, but depending on the decisions made during the game, more might be added or some might be lost.\n",
    "\n",
    "Each crewmember has a personal thirst and hunger gauge, which initially starts with 20 units and gets deducted by 1 each turn. If either one of these gauges reaches zero for one of the crewmembers, that specific crewmember will leave the spaceship. To prevent this from happening, the player can feed the crew during the Meal events, if there is enough food and water available.\n",
    "\n",
    "#### Random events\n",
    "\n",
    "Every turn, the player gets to make a decision based on an event selected at random out of 6 possibilities. These are:\n",
    "\n",
    "    Space Station Event: In this event, the ship encounters a Space Station which offers to replenish one of the four main resources. The player has to select between two of said resources, selected at random.\n",
    "    Pirate Attack Event: Here, a fleet of space pirates wanders near the ship. The player has to make a decision on whether to spend some gas to safely escape, or not use any fuel and run the risk of getting caught by the pirates, in which case a random resource level is lowered.\n",
    "    Trade Event: In this one, another spaceship appears and offers to trade one resource for another. The decision lies in either accepting the trade or not.\n",
    "    Supply Beacon Event: The spaceship stumbles upon a supply beacon, which contain one of the main resources, and the player gets to option to either open it or leave it. There are no possible harmful outcomes of opening the beacon.\n",
    "    SOS Call Event: A sudden SOS message is received by the crewmembers from a nearby stranded ship. If the player decides to help the stranded ship's crew, they will join the spaceship.\n",
    "    Planet Exploration Event: Finally, for this event the spaceship gets in range of a planet, and some crewmates offer to go look for resources. If the player sends the expedition, two outcomes might happen: The crewmembers might return with one resource, or they could get lost in the planet.\n",
    "\n",
    "#### Periodic events\n",
    "\n",
    "Besides the random events that happen every turn, there are 2 events that happen at fixated intervals and involve a different kind of choice from the player. These are the Propulsion Method Choice, and the Meal Event:\n",
    "\n",
    "    Propulsion Method Choice: Every 5 turns, the player is asked to select one of the 3 different propulsion methods: Standard, Electrical and Gas. If the standard method is selected, until the next time this event is triggered, the spaceship will use 2 units of power and 2 of gas as fuel. If the Electrical or Gas methods are selected, 4 units of power or gas, respectively, will be used.\n",
    "    Meal Event: Every 6 turns, the player can decide if the crewmembers should go eat. If the player decides to make them eat, 6 units of water and food will be lowered and fed to each of the crewmembers, within the capabilities of the tripulation. This means that, if the player runs out of water of food midway through this event, the rest of the crewmembers that didn't get to eat will keep their current thirst and hunger levels.\n",
    "    \n",
    "#### Score system\n",
    "The performance of the player gets determined by the score they get during their playthrough. Each turn, the player gets 100 points for each of the crewmembers that are still in the ship, however if the player loses one of their crewmembers (either by failling to keep them fed or losing them in the Exploration event) 500 points will be substracted from their score. \n",
    "\n",
    "#### Game over\n",
    "Finally, the game ends when one of three conditions are met: The ship runs out of power, runs out of gas, or loses all the members of its crew. Running out of food or water won't result in a game over, but will obviously mean that the player is at risk of losing their crew to hunger and thirst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first cell contains imports of the libraries that will be used throughout the program. The featured libraries are:\n",
    "    \n",
    "    The _random_ library, which comes included in Python\n",
    "    The _matplotlib_ library (https://matplotlib.org/users/installing.html), used to display graphs for trained agent's performance\n",
    "    The _tensorflow_ library (https://www.tensorflow.org/install), which contains all of the machine learning functions used for this program.\n",
    "    The _numpy_ library (https://numpy.org/install/), which is used by tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import abc\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.agents.reinforce import reinforce_agent\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell contains some definitions for several of the constants used by the Game environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAS_LEVEL = 100\n",
    "POWER_LEVEL = 100\n",
    "FOOD_LEVEL = 100\n",
    "WATER_LEVEL = 100\n",
    "CREW_NUMBER = 6 # This is the initial number of crewmembers\n",
    "FOOD_NEED = 20\n",
    "WATER_NEED = 20\n",
    "PROB_STATION = 0.045\n",
    "PROB_ATTACK = 0.05\n",
    "PROB_BEACON = 0.035\n",
    "PROB_TRADE = 0.06\n",
    "PROB_EXPLORATION = 0.05\n",
    "PROB_SOS = 0.035"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next cell, we define the Python Environment that will be passed to the agent to train in. This code is based on the one found in Tensorflow's official documentation, which can be found here: https://www.tensorflow.org/agents/tutorials/2_environments_tutorial, which was adapted to suit the specific requirements for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameEnv(py_environment.PyEnvironment):\n",
    "    \n",
    "    def __init__(self):\n",
    "        #For the action array, 0-food/water decision, 1-propulsion decision, 2-event decision\n",
    "        self._action_spec = array_spec.BoundedArraySpec(shape=(3,), dtype=np.int32, minimum=0, maximum=2,\n",
    "                                                        name='action')\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(shape=(12,), dtype=np.int32, name='observation')\n",
    "        self._game = Game()\n",
    "        self._episode_ended = False\n",
    "        self.turn = 0\n",
    "        self.crewmates = Crewmates()\n",
    "        \n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "    \n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "    \n",
    "    def _reset(self):\n",
    "        self._game.reset_game()\n",
    "        self.crewmates.reset_crewmates()\n",
    "        self._episode_ended = False\n",
    "        self.turn+=1\n",
    "        return ts.restart(np.array([self._game.resources[0], self._game.resources[2], self._game.resources[3],\n",
    "                                    self._game.resources[1], self._game.crew, self._game.selected_event,\n",
    "                                    self._game.options_station_event[0], self._game.options_station_event[1],\n",
    "                                    self._game.resources_trade_event[0], self._game.resources_trade_event[1],\n",
    "                                    self._game.crew_exploration_event, self._game.crew_sos_event], dtype=np.int32))\n",
    "    \n",
    "    def _step(self, action):\n",
    "        if self._episode_ended:\n",
    "            return self.reset()\n",
    "        \n",
    "        if not self._episode_ended:\n",
    "            if self.turn % 5 == 0 and self.turn != 0:\n",
    "                self._game.change_mode(action[1])\n",
    "\n",
    "            if self.turn % 6 == 0 and self.turn != 0:\n",
    "                self._game.meal(action[0], self.crewmates)   \n",
    "\n",
    "            self._game.check_events(self.crewmates, action[2])\n",
    "\n",
    "            self._game.ship()\n",
    "        \n",
    "        for i in range(len(self.crewmates.crewmates_left)):\n",
    "            self.crewmates.run_crewmates(i, self._game)\n",
    "            \n",
    "        if self._game.resources[0] <= 0:\n",
    "            self._episode_ended = True\n",
    "        if self._game.resources[1] <= 0:\n",
    "            self._episode_ended = True\n",
    "        if self._game.crew == 0:\n",
    "            self._episode_ended = True\n",
    "\n",
    "        if self._episode_ended:\n",
    "            reward = self._game.points\n",
    "            return ts.termination(np.array([self._game.resources[0], self._game.resources[2], self._game.resources[3],\n",
    "                                            self._game.resources[1], self._game.crew, self._game.selected_event,\n",
    "                                            self._game.options_station_event[0], self._game.options_station_event[1],\n",
    "                                            self._game.resources_trade_event[0], self._game.resources_trade_event[1],\n",
    "                                            self._game.crew_exploration_event, self._game.crew_sos_event], \n",
    "                                           dtype=np.int32), reward)\n",
    "        \n",
    "        else:\n",
    "            for i in range(len(self.crewmates.crewmates_left)):\n",
    "                if self.crewmates.crewmates_left[i] == False:\n",
    "                    self._game.points += 100\n",
    "                    \n",
    "            self.turn += 1\n",
    "                    \n",
    "            self._game.select_events()\n",
    "                \n",
    "            return ts.transition(np.array([self._game.resources[0], self._game.resources[2], self._game.resources[3],\n",
    "                                           self._game.resources[1], self._game.crew, self._game.selected_event,\n",
    "                                           self._game.options_station_event[0], self._game.options_station_event[1],\n",
    "                                           self._game.resources_trade_event[0], self._game.resources_trade_event[1],\n",
    "                                           self._game.crew_exploration_event, self._game.crew_sos_event],\n",
    "                                          dtype=np.int32), \n",
    "                                 reward=0.0, discount=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell contains the Game environment. In here is defined a the class _Game_ which will be called by the Python environment and which contains the functions and variables that control the game that will be played by the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game(object):\n",
    "    def __init__(self):\n",
    "        self.resources = [GAS_LEVEL, POWER_LEVEL, FOOD_LEVEL, WATER_LEVEL] # 0-gas, 1-power, 2-food, 3-water\n",
    "        self.crew = CREW_NUMBER\n",
    "        self.chance_station = PROB_STATION\n",
    "        self.chance_attack = PROB_ATTACK\n",
    "        self.chance_beacon = PROB_BEACON\n",
    "        self.chance_trade = PROB_TRADE\n",
    "        self.chance_exploration = PROB_EXPLORATION\n",
    "        self.chance_sos = PROB_SOS\n",
    "        self.control_mode = 0\n",
    "        self.points = 0\n",
    "        self.events_prob = [0, 0, 0, 0, 0, 0] # [station, attack, beacon, trade, exploration, sos]\n",
    "        self.options_station_event = [0,0] # 1 = power, 2 = gas, 3 = food, 4 = water\n",
    "        self.resources_trade_event = [0,0] # [resource requested, resource offered]\n",
    "        self.crew_exploration_event = 0\n",
    "        self.crew_sos_event = 0\n",
    "        self.selected_event = None\n",
    "        self.select_events()\n",
    "        \n",
    "    def select_events(self):\n",
    "        station = random.random()\n",
    "        attack = random.random()\n",
    "        beacon = random.random()\n",
    "        trade = random.random()\n",
    "        exploration = random.random()\n",
    "        sos = random.random()\n",
    "        \n",
    "        events_selected = 0\n",
    "        \n",
    "        if station < self.chance_station:\n",
    "            self.events_prob[0] = 1\n",
    "            events_selected += 1\n",
    "        else:\n",
    "            self.events_prob[0] = 0\n",
    "            \n",
    "        if attack < self.chance_attack:\n",
    "            self.events_prob[1] = 1\n",
    "            events_selected += 1\n",
    "        else:\n",
    "            self.events_prob[1] = 0\n",
    "            \n",
    "        if beacon < self.chance_beacon:\n",
    "            self.events_prob[2] = 1\n",
    "            events_selected += 1\n",
    "        else:\n",
    "            self.events_prob[2] = 0\n",
    "            \n",
    "        if trade < self.chance_trade:\n",
    "            self.events_prob[3] = 1\n",
    "            events_selected += 1\n",
    "        else:\n",
    "            self.events_prob[3] = 0\n",
    "            \n",
    "        if exploration < self.chance_exploration:\n",
    "            self.events_prob[4] = 1\n",
    "            events_selected += 1\n",
    "        else:\n",
    "            self.events_prob[4] = 0\n",
    "            \n",
    "        if sos < self.chance_sos:\n",
    "            self.events_prob[5] = 1\n",
    "            events_selected += 1\n",
    "        else:\n",
    "            self.events_prob[5] = 0\n",
    "            \n",
    "        if events_selected != 0 and events_selected != 1:\n",
    "            events_to_randomize = []\n",
    "            for i in range(len(self.events_prob)):\n",
    "                if self.events_prob[i] == 1:\n",
    "                    events_to_randomize.append(i)\n",
    "                    \n",
    "            randomize = random.randint(0, len(events_to_randomize)-1)\n",
    "            \n",
    "            for i in range(len(self.events_prob)):\n",
    "                if i != events_to_randomize[randomize]:\n",
    "                    self.events_prob[i] = 0\n",
    "                    \n",
    "            events_to_randomize.clear()\n",
    "            \n",
    "        for i in range(len(self.events_prob)):\n",
    "            if self.events_prob[i] == 1:\n",
    "                self.selected_event = i\n",
    "                \n",
    "        if self.selected_event == None:\n",
    "            self.select_events()\n",
    "        \n",
    "        self.options_station_event = [0,0]\n",
    "        self.resources_trade_event = [0,0] \n",
    "        self.crew_exploration_event = 0\n",
    "        self.crew_sos_event = 0\n",
    "            \n",
    "        self.select_event_random_values(self.selected_event)\n",
    "        \n",
    "    def select_event_random_values(self, event):\n",
    "        #station event\n",
    "        if event == 0:\n",
    "            self.options_station_event[0] = random.randint(1, 4)\n",
    "            self.options_station_event[1] = random.randint(1, 4)\n",
    "            \n",
    "            while self.options_station_event[1] == self.options_station_event[0]:\n",
    "                self.options_station_event[1] = random.randint(0, 3)\n",
    "        \n",
    "        #trade event\n",
    "        elif event == 3:\n",
    "            self.resources_trade_event[0] = random.randint(1, 4)\n",
    "            self.resources_trade_event[1] = random.randint(1, 4)\n",
    "            \n",
    "            while self.resources_trade_event[1] == self.resources_trade_event[0]:\n",
    "                self.resources_trade_event[1] = random.randint(1, 4)\n",
    "        \n",
    "        #exploration event\n",
    "        elif event == 4:\n",
    "            self.crew_exploration_event = random.randint(1, 4)\n",
    "            while self.crew - self.crew_exploration_event < 0:\n",
    "                self.crew_exploration_event = 0\n",
    "                self.crew_exploration_event = random.randint(1, 4)\n",
    "            \n",
    "        #sos event\n",
    "        elif event == 5:\n",
    "            self.crew_sos_event = random.randint(1, 5)\n",
    "    \n",
    "    def check_events(self, crewmates, action):\n",
    "        if self.events_prob[0] == 1:\n",
    "            self.station_event(action)\n",
    "            \n",
    "        if self.events_prob[1] == 1:\n",
    "            self.attack_event(action)\n",
    "            \n",
    "        if self.events_prob[2] == 1:\n",
    "            self.beacon_event(action)\n",
    "            \n",
    "        if self.events_prob[3] == 1:\n",
    "            self.trade_event(action)\n",
    "            \n",
    "        if self.events_prob[4] == 1:\n",
    "            self.exploration_event(action, crewmates)\n",
    "            \n",
    "        if self.events_prob[5] == 1:\n",
    "            self.sos_event(action, crewmates)\n",
    "    \n",
    "    # Controls the power and gas depletion\n",
    "    def ship(self):\n",
    "        if self.control_mode == 0:\n",
    "            self.resources[1] -= 2\n",
    "            self.resources[0] -= 2\n",
    "            return\n",
    "            \n",
    "        elif self.control_mode == 1:\n",
    "            self.resources[0] -= 4\n",
    "            return\n",
    "            \n",
    "        elif self.control_mode == 2:\n",
    "            self.resources[1] -= 4\n",
    "            return\n",
    "            \n",
    "    def change_mode(self, action):\n",
    "        if action == 0:\n",
    "            self.control_mode = 0\n",
    "            return\n",
    "\n",
    "        elif action == 1:\n",
    "            self.control_mode = 1\n",
    "            return\n",
    "\n",
    "        else:\n",
    "            self.control_mode = 2\n",
    "            return\n",
    "                \n",
    "    # Code for supply replenishment event\n",
    "    def station_event(self, action):\n",
    "        if action == 0:\n",
    "            action = self.options_station_event[0]\n",
    "            \n",
    "        else:\n",
    "            action = self.options_station_event[1]\n",
    "            \n",
    "        refill = random.randint(20, 50)\n",
    "            \n",
    "        self.update_post_event(action - 1, refill)\n",
    "            \n",
    "        return\n",
    "        \n",
    "    # Code for space pirate attack event\n",
    "    def attack_event(self, action):\n",
    "        if action == 0:\n",
    "            depletion = random.randint(15, 35)\n",
    "            self.resources[0] -= depletion\n",
    "            return\n",
    "                \n",
    "        else:\n",
    "            escape_prob = 0.4\n",
    "            escape_try = random.random()\n",
    "            \n",
    "            if escape_prob >= escape_try:\n",
    "                return\n",
    "                \n",
    "            else:\n",
    "                supply_lost = random.randint(0, 3)\n",
    "                amount_lost = -1 * random.randint(30, 50)\n",
    "                \n",
    "                self.update_post_event(supply_lost, amount_lost)\n",
    "            \n",
    "                return\n",
    "        \n",
    "    # Code for supply beacon event\n",
    "    def beacon_event(self, action):        \n",
    "        if action == 0:\n",
    "            supply = random.randint(0, 3)\n",
    "            amount_found = random.randint(10, 50)\n",
    "            \n",
    "            self.update_post_event(supply, amount_found)\n",
    "            \n",
    "            return\n",
    "                \n",
    "        else:\n",
    "            return\n",
    "\n",
    "    # Code for trade event\n",
    "    def trade_event(self, action):\n",
    "        amount_offered = random.randint(30, 60)\n",
    "        amount_requested = random.randint(10, 40)\n",
    "        \n",
    "        if action == 0:\n",
    "            self.update_post_event(self.resources_trade_event[1] - 1, amount_offered)\n",
    "            self.update_post_event(self.resources_trade_event[0] - 1, -1 * amount_requested)\n",
    "            return\n",
    "                \n",
    "        else:\n",
    "            return\n",
    "            \n",
    "    # Code for exploration event\n",
    "    def exploration_event(self, action, crewmates):\n",
    "        sucess_chance = 0.65\n",
    "        \n",
    "        if action == 0:\n",
    "            exploration_result = random.random()\n",
    "            if sucess_chance > exploration_result:\n",
    "                found = random.randint(0, 3)\n",
    "                amount_found = random.randint(50, 80)\n",
    "                self.update_post_event(found, amount_found)\n",
    "                return\n",
    "            \n",
    "            else:\n",
    "                crewmates.lost_in_planet(self.crew_exploration_event)\n",
    "                for i in range(self.crew_exploration_event):\n",
    "                    self.crew -= 1\n",
    "                    self.points -= 500\n",
    "                return\n",
    "                \n",
    "        else:\n",
    "            return\n",
    "\n",
    "    # Code for sos call event\n",
    "    def sos_event(self, action, crewmates):     \n",
    "        if action == 0:\n",
    "            new_crew = self.crew + self.crew_sos_event\n",
    "            \n",
    "            for i in range(self.crew, new_crew):\n",
    "                crewmates.crewmate()\n",
    "                \n",
    "            self.crew = new_crew\n",
    "            \n",
    "            return\n",
    "                \n",
    "        else:\n",
    "            return\n",
    "    \n",
    "    # Code for meal event\n",
    "    def meal(self, action, crewmates):\n",
    "        if action == 0:\n",
    "            for i in range(len(crewmates.crewmates_left)):\n",
    "                if not crewmates.crewmates_left[i]:\n",
    "                    self.give_food(i, crewmates)\n",
    "                    self.give_water(i, crewmates)\n",
    "            return\n",
    "        \n",
    "        else:\n",
    "            return\n",
    "        \n",
    "    # Code for giving food to the crew\n",
    "    def give_food(self, name, crewmates):\n",
    "        if self.resources[2] > 5:\n",
    "            self.resources[2] -= 6\n",
    "            crewmates.crewmates_food[name] += 6\n",
    "        return\n",
    "            \n",
    "    # Code for giving water to the crew\n",
    "    def give_water(self, name, crewmates):\n",
    "        if self.resources[3] > 5:\n",
    "            self.resources[3] -= 6\n",
    "            crewmates.crewmates_water[name] += 6\n",
    "        return\n",
    "    \n",
    "    def update_post_event(self, resource, amount):\n",
    "        if resource == 0:\n",
    "            self.resources[0] += amount\n",
    "\n",
    "        elif resource == 1:\n",
    "            self.resources[1] += amount\n",
    "\n",
    "        elif resource == 2:\n",
    "            self.resources[2] += amount\n",
    "            if self.resources[2] < 0:\n",
    "                self.resources[2] = 0\n",
    "\n",
    "        elif resource == 3:\n",
    "            self.resources[3] += amount\n",
    "            if self.resources[3] < 0:\n",
    "                self.resources[3] = 0\n",
    "                \n",
    "    def reset_game(self):\n",
    "        self.resources = [GAS_LEVEL, POWER_LEVEL, FOOD_LEVEL, WATER_LEVEL]\n",
    "        self.crew = CREW_NUMBER\n",
    "        self.chance_station = PROB_STATION\n",
    "        self.chance_attack = PROB_ATTACK\n",
    "        self.chance_beacon = PROB_BEACON\n",
    "        self.chance_trade = PROB_TRADE\n",
    "        self.chance_exploration = PROB_EXPLORATION\n",
    "        self.chance_sos = PROB_SOS\n",
    "        self.control_mode = 0\n",
    "        self.points = 0\n",
    "        self.events_prob = [0, 0, 0, 0, 0, 0]\n",
    "        self.options_station_event = [0,0]\n",
    "        self.resources_trade_event = [0,0]\n",
    "        self.crew_exploration_event = 0\n",
    "        self.crew_sos_event = 0\n",
    "        self.selected_event = None\n",
    "        self.select_events()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cells defines the class _Crewmates_ which contains the code that controls the crewmates' behaviour: stores a record of each crewmate's water and food levels, and defines the functions used to generate a new crewmember, update their resource need everyturn and remove crewmates if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crewmates(object):\n",
    "    def __init__(self):\n",
    "        self.crewmates_food = []\n",
    "        self.crewmates_water = []\n",
    "        self.crewmates_left = []\n",
    "        for i in range(CREW_NUMBER):\n",
    "            self.crewmate()\n",
    "        \n",
    "    # Generates a crewmate\n",
    "    def crewmate(self):\n",
    "        self.crewmates_food.append(FOOD_NEED)\n",
    "        self.crewmates_water.append(WATER_NEED)\n",
    "        self.crewmates_left.append(False)\n",
    "    \n",
    "    # Updates each crewmate's food and water levels\n",
    "    def run_crewmates(self, name, game):\n",
    "        if not self.crewmates_left[name]:\n",
    "            if self.crewmates_food[name] == 0:\n",
    "                game.crew -= 1\n",
    "                self.crewmates_left[name] = True\n",
    "                game.points -= 500\n",
    "                return\n",
    "\n",
    "            if self.crewmates_water[name] == 0:\n",
    "                game.crew -= 1\n",
    "                self.crewmates_left[name] = True\n",
    "                game.points -= 500\n",
    "                return\n",
    "\n",
    "            self.crewmates_food[name] -= 1\n",
    "            self.crewmates_water[name] -= 1\n",
    "\n",
    "        if self.crewmates_left[name]:\n",
    "            return\n",
    "\n",
    "    # Code for crewmates lost in exploration event\n",
    "    def lost_in_planet(self, number):\n",
    "        pool = []\n",
    "        \n",
    "        for i in range(len(self.crewmates_left)):\n",
    "            if not self.crewmates_left[i]:\n",
    "                pool.append(i)\n",
    "        \n",
    "        len_pool = len(pool)\n",
    "        for i in range(number):\n",
    "            name = random.randint(0, len_pool - 1)\n",
    "            while self.crewmates_left[pool[name]] == True:\n",
    "                name = random.randint(0, len_pool - 1)\n",
    "\n",
    "            self.crewmates_left[pool[name]] = True\n",
    "                        \n",
    "    def reset_crewmates(self):\n",
    "        self.crewmates_food = []\n",
    "        self.crewmates_water = []\n",
    "        self.crewmates_left = []\n",
    "        for i in range(CREW_NUMBER):\n",
    "            self.crewmate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the environment is coded, it is necesary to test it using the tools provided by Tensorflow. In the next cell, the validate_py_environment is used to iterate through 5 episodes of the environment created using a random policy. If we have made any mistake with our code (on the environment definition or on the classes), an error message will appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "environment = GameEnv()\n",
    "utils.validate_py_environment(environment, episodes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define two environments, one will be used to train the agent and the second will be used to test the trained agent.\n",
    "\n",
    "In here, we are translating our Python Environment to a Tensorflow Environment using TfPyEnvironment. What this does is basically transform the numpy arrays to _Tensors_, which makes it easier for the user to interact with the policies and the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = tf_py_environment.TFPyEnvironment(GameEnv())\n",
    "eval_env = tf_py_environment.TFPyEnvironment(GameEnv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have defined and checked that our environment is worked as intended, we can go to the agent code. \n",
    "\n",
    "For the next cells, the code used is an adaptation of the code on this Tensorflow's documentation webpage: https://www.tensorflow.org/agents/tutorials/6_reinforce_tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define some hyperparameters that will be used throughout the coding of the agent.\n",
    "\n",
    "Tweaking the hyperparameters is a very delicate part of the process of training an AI, as finding the optimal values for each of the parameters can make a huge different on the extend to which the AI learns, the time it takes to do so, the resources it takes to run, and so on. \n",
    "\n",
    "The values present in this code were found through trial and error: Some parameters were changed, then the agent was run, then the results were compared with others using different configurations, then repeat. And while these are the values that were found to work the best, that doesn't mean they are the most optimal configuration; maybe changing some could achieve higher results or lower runtimes (while obviously negatively impacting other metric), so feel free to experiment if you want.\n",
    "\n",
    "To make the search for new configurations easier, what follows is a brief description of what each of these hyperparameters do:\n",
    "\n",
    "    num_iterations: The AI learns to play by coursing through a number of iterations, each comprised of a set number of episodes. Having more iterations might allow the AI to learn more by having more experience with the game, but it will also make the runtime increase.\n",
    "    \n",
    "    collect_episodes_per_iteration: Each episode refers to a playthrough of the game: Starts by creating a new instance of  the game, and ends when the AI gets a Game Over. Same as with before, augmenting this number will give the AI more experience, at a runtime cost.\n",
    "    \n",
    "    replay_buffer_capacity: The AI needs to store its results in a buffer to properly learn. This parameter refers to the   maximum capacity of said buffer. Having more buffer capacity will allow the AI to store more information of its previous runs, but might put strain on your computer.\n",
    "    \n",
    "    fc_layer_params: This hyperparameter defines the \"network\" of the AI; it's an array where each value refers to the number of nodes on the layer of the index of its position, e.g.: inputting (1, 50, 25) would represent one node on its first layer, fifty on its second and twenty five on its third. Both having more layers and nodes might improve the learning capability of the agent, but will again augment the runtime. Also, it is worth noting that once arrived to certain values adding more layers or nodes won't have a significant effect on the agent, and might even be counterproductive. Optimal values can be reached by just changing this value, but researching external documentation might be helpful.\n",
    "    \n",
    "    learning_rate: This hyperparameter is one of the hardest to change effectively, and before changing it you might want to research at least the basics of how this works. Still, an oversimplification might be the following: The agent learns by trying different solutions when given a certain problem. This solutions can be illustrated as points in an axis, and the AI will jump from solution to solution in search for the one that best suits the problem. In this scenario, the learning rate is how far this \"jumps\" are. So having a value too big might make the agent come with the answer faster, but at the risk of entirely missing it; while on the other extreme, having a value too small might ensure that the answer is reached, but the runtime would be enormous.\n",
    "    \n",
    "    log_interval: When training, the agent will periodically record an average of the results obtained the last iterations. This hyperparameter defines how often that log happens.\n",
    "    \n",
    "    num_eval_episodes: Every so often, the agent will submit itself to an evaluation where its efficacy is gauged. This hyperparameter changes the number of episodes in the evaluation. Greater values will give more information on the performance of the agent, at some cost of runtime.\n",
    "    \n",
    "    eval_interval: This hyperparameter affects how often the evaluation happens. While having more evaluations might help the agent to gauge it performance more often, it is also important to have a significant number of training iterations before arriving to a evaluation, to give the agent time to change its approach to the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 1000 # @param {type:\"integer\"}\n",
    "collect_episodes_per_iteration = 20 # @param {type:\"integer\"}\n",
    "replay_buffer_capacity = 100000 # @param {type:\"integer\"}\n",
    "\n",
    "fc_layer_params = (50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,)\n",
    "\n",
    "learning_rate = 1e-3 # @param {type:\"number\"}\n",
    "log_interval = 25 # @param {type:\"integer\"}\n",
    "num_eval_episodes = 10 # @param {type:\"integer\"}\n",
    "eval_interval = 50 # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing needed to code our actor is an Actor Network, tasked with predicting an action given a certain observation from the environment. We pass our observation and action's spec, as well as _fc_layer_params_ which is a tuple of ints, each representing the number of nodes for the hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define an optimizer for the created network. Tensorflow has a variety of diferent optimizers, here the Adam Optimization Algorithm is used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.compat.v2.Variable(0)\n",
    "\n",
    "tf_agent = reinforce_agent.ReinforceAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    actor_network=actor_net,\n",
    "    optimizer=optimizer,\n",
    "    normalize_returns=True,\n",
    "    train_step_counter=train_step_counter)\n",
    "tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A policy represents an action to take on a specific timestep, as a response to an observation from the environment.\n",
    "\n",
    "The agents contain two policies, one is used to for the training and later is the policy used by the trained agent (evaluation policy), and one is used to collect data (collect policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = tf_agent.policy\n",
    "collect_policy = tf_agent.collect_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also important to define a return to evaluate how good the policy is. The one used by the Tensorflow documentation is the average return policy, and is one of the most commonly used. It works by computing an average the rewards over a set number of episodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The replay buffer allows the agent to store the data from its previous episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=tf_agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_capacity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a function that allows the agent to store the results for a given episode in the replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_episode(environment, policy, num_episodes):\n",
    "\n",
    "    episode_counter = 0\n",
    "    environment.reset()\n",
    "\n",
    "    while episode_counter < num_episodes:\n",
    "        time_step = environment.current_time_step()\n",
    "        action_step = policy.action(time_step)\n",
    "        next_time_step = environment.step(action_step.action)\n",
    "        traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "        replay_buffer.add_batch(traj)\n",
    "\n",
    "        if traj.is_boundary():\n",
    "            episode_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this we have all the pieces needed to actually train the agent, so we pass to the actual training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    %%time\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "tf_agent.train = common.function(tf_agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "tf_agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "    # Collect a few episodes using collect_policy and save to the replay buffer.\n",
    "    collect_episode(train_env, tf_agent.collect_policy, collect_episodes_per_iteration)\n",
    "\n",
    "    # Use data from the buffer and update the agent's network.\n",
    "    experience = replay_buffer.gather_all()\n",
    "    train_loss = tf_agent.train(experience)\n",
    "    replay_buffer.clear()\n",
    "\n",
    "    step = tf_agent.train_step_counter.numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss.loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These final two cells contain the code needed to visualize what we have just trained.\n",
    "\n",
    "The first one contains code to put into a matplotlib plot the results obtained throughout the previous training, and helps primarily by telling the user the performance of the agent during training, thus providing helpful information for the process of selecting the best hyperparameter configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(steps, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Step')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second and final cell of the project contains a test run for the trained agent, allowing us to see how the  agent applies the policies made during training to a general scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bot_evaluation(policy, num_episodes=5):\n",
    "    for _ in range(num_episodes):\n",
    "        print(\"Episode: {}\".format(_))\n",
    "        time_step = eval_env.reset()\n",
    "        while not time_step.is_last():\n",
    "            print(time_step)\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = eval_env.step(action_step.action)\n",
    "        print(\"Final Score: {}\".format(time_step.reward))\n",
    "        print(\"------------------------------------------------------------------------------------------\")\n",
    "\n",
    "bot_evaluation(tf_agent.policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
